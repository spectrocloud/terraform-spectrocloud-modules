name: libvirt-gpu-full
description: ""
type: cluster
cloudType: libvirt
packs:
  - name: ubuntu-libvirt
    type: spectro
    layer: os
    registry: Public Repo
    version: 20.04
    tag: 20.04
    values: |-
      # Spectro Golden images includes most of the hardening standards recommended by CIS benchmarking v1.5

      # Uncomment below section to
      # 1. Include custom files to be copied over to the nodes and/or
      # 2. Execute list of commands before or after kubeadm init/join is executed
      #
      kubeadmconfig:
        preKubeadmCommands:
        - echo "Executing pre kube admin config commands"
        - update-ca-certificates
        postKubeadmCommands:
        - echo "Executing post kube admin config commands"
        - mkdir -p /etc/containerd/conf.d/
        files:
        - targetPath: /usr/local/share/ca-certificates/ca.crt
          targetOwner: "root:root"
          targetPermissions: "0644"
          content: |
            -----BEGIN CERTIFICATE-----
            MIIDozCCAougAwIBAgIQeO8XlqAMLhxvtCap35yktzANBgkqhkiG9w0BAQsFADBS
            MQswCQYDVQQGEwJVUzEhMB8GA1UEChMYR2VuZXJhbCBFbGVjdHJpYyBDb21wYW55
            MSAwHgYDVQQDExdHRSBFeHRlcm5hbCBSb290IENBIDIuMTAeFw0xNTAzMDUwMDAw
            MDBaFw0zNTAzMDQyMzU5NTlaMFIxCzAJBgNVBAYTAlVTMSEwHwYDVQQKExhHZW5l
            cmFsIEVsZWN0cmljIENvbXBhbnkxIDAeBgNVBAMTF0dFIEV4dGVybmFsIFJvb3Qg
            Q0EgMi4xMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAzCzT4wNRZtr2
            XTzoTMjppjulZfG35/nOt44q2zg47sxwgZ8o4qjcrwzIhsntoFrRQssjXSF5qXdC
            zsm1G7f04qEBimuOH/X+CidWX+sudCS8VyRjXi9cyvUW4/mYKCLXv5M6HhEoIHCD
            Xdo6yUr5mSrf18qRR3yUFz0HYXopa2Ls3Q6lBvEUO2Xw04vqVvmg1h7S5jYuZovC
            oIbd2+4QGdoSZPgtSNpCxSR+NwtPpzYZpmqiUuDGfVpO3HU42APB0c60D91cJho6
            tZpXYHDsR/RxYGm02K/iMGefD5F4YMrtoKoHbskty6+u5FUOrUgGATJJGtxleg5X
            KotQYu8P1wIDAQABo3UwczASBgNVHRMBAf8ECDAGAQH/AgECMA4GA1UdDwEB/wQE
            AwIBBjAuBgNVHREEJzAlpCMwITEfMB0GA1UEAxMWR0UtUm9vdC1DT00tUlNBLTIw
            NDgtMTAdBgNVHQ4EFgQU3N2mUCJBCLYgtpZyxBeBMJwNZuowDQYJKoZIhvcNAQEL
            BQADggEBACF4Zsf2Nm0FpVNeADUH+sl8mFgwL7dfL7+6n7hOgH1ZXcv6pDkoNtVE
            0J/ZPdHJW6ntedKEZuizG5BCclUH3IyYK4/4GxNpFXugmWnKGy2feYwVae7Puyd7
            /iKOFEGCYx4C6E2kq3aFjJqiq1vbgSS/B0agt1D3rH3i/+dXVxx8ZjhyZMuN+cgS
            pZL4gnhnSXFAGissxJhKsNkYgvKdOETRNn5lEgfgVyP2iOVqEguHk2Gu0gHSouLu
            5ad/qyN+Zgbjx8vEWlywmhXb78Gaf/AwSGAwQPtmQ0310a4DulGxo/kcuS78vFH1
            mwJmHm9AIFoqBi8XpuhGmQ0nvymurEk=
            -----END CERTIFICATE-----
  - name: kubernetes
    type: spectro
    layer: k8s
    registry: Public Repo
    version: 1.21.8
    tag: 1.21.8
    values: |-
      pack:
        k8sHardening: True
        #CIDR Range for Pods in cluster
        # Note : This must not overlap with any of the host or service network
        podCIDR: "172.10.0.0/16"
        #CIDR notation IP range from which to assign service cluster IPs
        # Note : This must not overlap with any IP ranges assigned to nodes for pods.
        serviceClusterIpRange: "11.0.0.0/22"

      # KubeAdm customization for kubernetes hardening. Below config will be ignored if k8sHardening property above is disabled
      kubeadmconfig:
        apiServer:
          certSANs:
          - "cluster-{{ .spectro.system.cluster.uid }}.{{ .spectro.system.reverseproxy.server }}"
          extraArgs:
            # Note : secure-port flag is used during kubeadm init. Do not change this flag on a running cluster
            secure-port: "6443"
            anonymous-auth: "true"
            insecure-port: "0"
            profiling: "false"
            disable-admission-plugins: "AlwaysAdmit"
            default-not-ready-toleration-seconds: "60"
            default-unreachable-toleration-seconds: "60"
            enable-admission-plugins: "NamespaceLifecycle,ServiceAccount,NodeRestriction,PodSecurityPolicy"
            audit-log-path: /var/log/apiserver/audit.log
            audit-policy-file: /etc/kubernetes/audit-policy.yaml
            audit-log-maxage: "30"
            audit-log-maxbackup: "10"
            audit-log-maxsize: "100"
            authorization-mode: RBAC,Node
            tls-cipher-suites: "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256"
          extraVolumes:
            - name: audit-log
              hostPath: /var/log/apiserver
              mountPath: /var/log/apiserver
              pathType: DirectoryOrCreate
            - name: audit-policy
              hostPath: /etc/kubernetes/audit-policy.yaml
              mountPath: /etc/kubernetes/audit-policy.yaml
              readOnly: true
              pathType: File
        controllerManager:
          extraArgs:
            profiling: "false"
            terminated-pod-gc-threshold: "25"
            pod-eviction-timeout: "1m0s"
            use-service-account-credentials: "true"
            feature-gates: "RotateKubeletServerCertificate=true"
        scheduler:
          extraArgs:
            profiling: "false"
        kubeletExtraArgs:
          read-only-port : "0"
          event-qps: "0"
          feature-gates: "RotateKubeletServerCertificate=true"
          protect-kernel-defaults: "true"
          tls-cipher-suites: "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256"
        files:
          - path: hardening/audit-policy.yaml
            targetPath: /etc/kubernetes/audit-policy.yaml
            targetOwner: "root:root"
            targetPermissions: "0600"
          - path: hardening/privileged-psp.yaml
            targetPath: /etc/kubernetes/hardening/privileged-psp.yaml
            targetOwner: "root:root"
            targetPermissions: "0600"
          - path: hardening/90-kubelet.conf
            targetPath: /etc/sysctl.d/90-kubelet.conf
            targetOwner: "root:root"
            targetPermissions: "0600"
        preKubeadmCommands:
          # For enabling 'protect-kernel-defaults' flag to kubelet, kernel parameters changes are required
          - 'echo "====> Applying kernel parameters for Kubelet"'
          - 'sysctl -p /etc/sysctl.d/90-kubelet.conf'
        postKubeadmCommands:
          # Apply the privileged PodSecurityPolicy on the first master node ; Otherwise, CNI (and other) pods won't come up
          # Sometimes api server takes a little longer to respond. Retry if applying the pod-security-policy manifest fails
          - 'export KUBECONFIG=/etc/kubernetes/admin.conf && [ -f "$KUBECONFIG" ] && { echo " ====> Applying PodSecurityPolicy" ; until $(kubectl apply -f /etc/kubernetes/hardening/privileged-psp.yaml > /dev/null ); do echo "Failed to apply PodSecurityPolicies, will retry in 5s" ; sleep 5 ; done ; } || echo "Skipping PodSecurityPolicy for worker nodes"'

      # Client configuration to add OIDC based authentication flags in kubeconfig
      #clientConfig:
        #oidc-issuer-url: "{{ .spectro.pack.kubernetes.kubeadmconfig.apiServer.extraArgs.oidc-issuer-url }}"
        #oidc-client-id: "{{ .spectro.pack.kubernetes.kubeadmconfig.apiServer.extraArgs.oidc-client-id }}"
        #oidc-client-secret: 1gsranjjmdgahm10j8r6m47ejokm9kafvcbhi3d48jlc3rfpprhv
        #oidc-extra-scope: profile,email
  - name: cni-calico
    type: spectro
    layer: cni
    registry: Public Repo
    version: 3.19.0
    tag: 3.19.0
    values: |-
      pack:
        content:
          images:
            - image: gcr.io/spectro-images-public/calico/kube-controllers:v3.19.0
            - image: gcr.io/spectro-images-public/calico/node:v3.19.0
            - image: gcr.io/spectro-images-public/calico/cni:v3.19.0
            - image: gcr.io/spectro-images-public/calico/pod2daemon-flexvol:v3.19.0

      manifests:
        calico:

          # IPAM type to use. Supported types are calico-ipam, host-local
          ipamType: "calico-ipam"

          # Should be one of CALICO_IPV4POOL_IPIP or CALICO_IPV4POOL_VXLAN
          encapsulationType: "CALICO_IPV4POOL_IPIP"

          # Should be one of Always, CrossSubnet, Never
          encapsulationMode: "Always"
  - name: csi-rook-ceph
    type: spectro
    layer: csi
    registry: Public Repo
    version: 1.8.0
    tag: 1.8.0
    values: |-
      pack:
        content:
          images:
            - image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.3.0
            - image: k8s.gcr.io/sig-storage/csi-attacher:v3.3.0
            - image: k8s.gcr.io/sig-storage/csi-snapshotter:v4.2.0
            - image: k8s.gcr.io/sig-storage/csi-resizer:v1.3.0
            - image: k8s.gcr.io/sig-storage/csi-provisioner:v3.0.0
            - image: quay.io/cephcsi/cephcsi:v3.4.0
            - image: quay.io/ceph/ceph:v16.2.7
            - image: docker.io/rook/ceph:v1.8.0

      manifests:
        storageclass:
          contents: |
            apiVersion: ceph.rook.io/v1
            kind: CephFilesystem
            metadata:
              name: myfs
              namespace: rook-ceph # namespace:cluster
            spec:
              # The metadata pool spec. Must use replication.
              metadataPool:
                replicated:
                  size: 3
                  requireSafeReplicaSize: true
                parameters:
                  # Inline compression mode for the data pool
                  # Further reference: https://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#inline-compression
                  compression_mode:
                    none
                  # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool
                  # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size
                  #target_size_ratio: ".5"
              # The list of data pool specs. Can use replication or erasure coding.
              dataPools:
                - name: replicated
                  failureDomain: host
                  replicated:
                    size: 3
                    # Disallow setting pool with replica 1, this could lead to data loss without recovery.
                    # Make sure you're *ABSOLUTELY CERTAIN* that is what you want
                    requireSafeReplicaSize: true
                  parameters:
                    # Inline compression mode for the data pool
                    # Further reference: https://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#inline-compression
                    compression_mode:
                      none
                    # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool
                    # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size
                    #target_size_ratio: ".5"
              # Whether to preserve filesystem after CephFilesystem CRD deletion
              preserveFilesystemOnDelete: true
              # The metadata service (mds) configuration
              metadataServer:
                # The number of active MDS instances
                activeCount: 1
                # Whether each active MDS instance will have an active standby with a warm metadata cache for faster failover.
                # If false, standbys will be available, but will not have a warm cache.
                activeStandby: true
                # The affinity rules to apply to the mds deployment
                placement:
                  #  nodeAffinity:
                  #    requiredDuringSchedulingIgnoredDuringExecution:
                  #      nodeSelectorTerms:
                  #      - matchExpressions:
                  #        - key: role
                  #          operator: In
                  #          values:
                  #          - mds-node
                  #  topologySpreadConstraints:
                  #  tolerations:
                  #  - key: mds-node
                  #    operator: Exists
                  #  podAffinity:
                  podAntiAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                      - labelSelector:
                          matchExpressions:
                            - key: app
                              operator: In
                              values:
                                - rook-ceph-mds
                        # topologyKey: kubernetes.io/hostname will place MDS across different hosts
                        topologyKey: kubernetes.io/hostname
                    preferredDuringSchedulingIgnoredDuringExecution:
                      - weight: 100
                        podAffinityTerm:
                          labelSelector:
                            matchExpressions:
                              - key: app
                                operator: In
                                values:
                                  - rook-ceph-mds
                          # topologyKey: */zone can be used to spread MDS across different AZ
                          # Use <topologyKey: failure-domain.beta.kubernetes.io/zone> in k8s cluster if your cluster is v1.16 or lower
                          # Use <topologyKey: topology.kubernetes.io/zone>  in k8s cluster is v1.17 or upper
                          topologyKey: topology.kubernetes.io/zone
                # A key/value list of annotations
                annotations:
                #  key: value
                # A key/value list of labels
                labels:
                #  key: value
                resources:
                  # The requests and limits set here, allow the filesystem MDS Pod(s) to use half of one CPU core and 1 gigabyte of memory
                  #  limits:
                  #    cpu: "500m"
                  #    memory: "1024Mi"
                  #  requests:
                  #    cpu: "500m"
                  #    memory: "1024Mi"
                  # priorityClassName: my-priority-class
                  # Filesystem mirroring settings
                  # mirroring:
                  # enabled: true
                  # list of Kubernetes Secrets containing the peer token
                  # for more details see: https://docs.ceph.com/en/latest/dev/cephfs-mirroring/#bootstrap-peers
                  # peers:
                  #secretNames:
                  #- secondary-cluster-peer
                  # specify the schedule(s) on which snapshots should be taken
                  # see the official syntax here https://docs.ceph.com/en/latest/cephfs/snap-schedule/#add-and-remove-schedules
                  # snapshotSchedules:
                  #   - path: /
                  #     interval: 24h # daily snapshots
                  #     startTime: 11:55
                  # manage retention policies
                  # see syntax duration here https://docs.ceph.com/en/latest/cephfs/snap-schedule/#add-and-remove-retention-policies
                  # snapshotRetention:
                  #   - path: /
                  #     duration: "h 24"
            ---
            apiVersion: storage.k8s.io/v1
            kind: StorageClass
            metadata:
              name: standard
              annotations:
                storageclass.kubernetes.io/is-default-class: "true"
            # Change "rook-ceph" provisioner prefix to match the operator namespace if needed
            provisioner: rook-ceph.cephfs.csi.ceph.com # driver:namespace:operator
            parameters:
              # clusterID is the namespace where the rook cluster is running
              # If you change this namespace, also change the namespace below where the secret namespaces are defined
              clusterID: rook-ceph # namespace:cluster

              # CephFS filesystem name into which the volume shall be created
              fsName: myfs

              # Ceph pool into which the volume shall be created
              # Required for provisionVolume: "true"
              pool: myfs-data0

              # The secrets contain Ceph admin credentials. These are generated automatically by the operator
              # in the same namespace as the cluster.
              csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
              csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph # namespace:cluster
              csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
              csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph # namespace:cluster
              csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
              csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph # namespace:cluster

              # (optional) The driver can use either ceph-fuse (fuse) or ceph kernel client (kernel)
              # If omitted, default volume mounter will be used - this is determined by probing for ceph-fuse
              # or by setting the default mounter explicitly via --volumemounter command-line argument.
              # mounter: kernel
            reclaimPolicy: Delete
            allowVolumeExpansion: true
            #Supported binding modes are Immediate, WaitForFirstConsumer
            volumeBindingMode: "WaitForFirstConsumer"
            mountOptions:
              # uncomment the following line for debugging
              #- debug

        cluster:
          contents: |
            apiVersion: ceph.rook.io/v1
            kind: CephCluster
            metadata:
              name: rook-ceph
              namespace: rook-ceph # namespace:cluster
            spec:
              cephVersion:
                # The container image used to launch the Ceph daemon pods (mon, mgr, osd, mds, rgw).
                # v15 is octopus, and v16 is pacific.
                # RECOMMENDATION: In production, use a specific version tag instead of the general v14 flag, which pulls the latest release and could result in different
                # versions running within the cluster. See tags available at https://hub.docker.com/r/ceph/ceph/tags/.
                # If you want to be more precise, you can always use a timestamp tag such quay.io/ceph/ceph:v16.2.7-20211208
                # This tag might not contain a new Ceph version, just security fixes from the underlying operating system, which will reduce vulnerabilities
                image: quay.io/ceph/ceph:v16.2.7
                # Whether to allow unsupported versions of Ceph. Currently `octopus` and `pacific` are supported.
                # Future versions such as `pacific` would require this to be set to `true`.
                # Do not set to true in production.
                allowUnsupported: false
              # The path on the host where configuration files will be persisted. Must be specified.
              # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.
              # In Minikube, the '/data' directory is configured to persist across reboots. Use "/data/rook" in Minikube environment.
              dataDirHostPath: /var/lib/rook
              # Whether or not upgrade should continue even if a check fails
              # This means Ceph's status could be degraded and we don't recommend upgrading but you might decide otherwise
              # Use at your OWN risk
              # To understand Rook's upgrade process of Ceph, read https://rook.io/docs/rook/latest/ceph-upgrade.html#ceph-version-upgrades
              skipUpgradeChecks: false
              # Whether or not continue if PGs are not clean during an upgrade
              continueUpgradeAfterChecksEvenIfNotHealthy: false
              # WaitTimeoutForHealthyOSDInMinutes defines the time (in minutes) the operator would wait before an OSD can be stopped for upgrade or restart.
              # If the timeout exceeds and OSD is not ok to stop, then the operator would skip upgrade for the current OSD and proceed with the next one
              # if `continueUpgradeAfterChecksEvenIfNotHealthy` is `false`. If `continueUpgradeAfterChecksEvenIfNotHealthy` is `true`, then opertor would
              # continue with the upgrade of an OSD even if its not ok to stop after the timeout. This timeout won't be applied if `skipUpgradeChecks` is `true`.
              # The default wait timeout is 10 minutes.
              waitTimeoutForHealthyOSDInMinutes: 10
              mon:
                # Set the number of mons to be started. Generally recommended to be 3.
                # For highest availability, an odd number of mons should be specified.
                count: 3
                # The mons should be on unique nodes. For production, at least 3 nodes are recommended for this reason.
                # Mons should only be allowed on the same node for test environments where data loss is acceptable.
                allowMultiplePerNode: false
              mgr:
                # When higher availability of the mgr is needed, increase the count to 2.
                # In that case, one mgr will be active and one in standby. When Ceph updates which
                # mgr is active, Rook will update the mgr services to match the active mgr.
                count: 1
                modules:
                  # Several modules should not need to be included in this list. The "dashboard" and "monitoring" modules
                  # are already enabled by other settings in the cluster CR.
                  - name: pg_autoscaler
                    enabled: true
              # enable the ceph dashboard for viewing cluster status
              dashboard:
                enabled: true
                # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
                # urlPrefix: /ceph-dashboard
                # serve the dashboard at the given port.
                # port: 8443
                # serve the dashboard using SSL
                ssl: true
              # enable prometheus alerting for cluster
              monitoring:
                # requires Prometheus to be pre-installed
                enabled: false
                # namespace to deploy prometheusRule in. If empty, namespace of the cluster will be used.
                # Recommended:
                # If you have a single rook-ceph cluster, set the rulesNamespace to the same namespace as the cluster or keep it empty.
                # If you have multiple rook-ceph clusters in the same k8s cluster, choose the same namespace (ideally, namespace with prometheus
                # deployed) to set rulesNamespace for all the clusters. Otherwise, you will get duplicate alerts with multiple alert definitions.
                rulesNamespace: rook-ceph
              network:
                # enable host networking
                #provider: host
                # enable the Multus network provider
                #provider: multus
                #selectors:
                  # The selector keys are required to be `public` and `cluster`.
                  # Based on the configuration, the operator will do the following:
                  #   1. if only the `public` selector key is specified both public_network and cluster_network Ceph settings will listen on that interface
                  #   2. if both `public` and `cluster` selector keys are specified the first one will point to 'public_network' flag and the second one to 'cluster_network'
                  #
                  # In order to work, each selector value must match a NetworkAttachmentDefinition object in Multus
                  #
                  #public: public-conf --> NetworkAttachmentDefinition object name in Multus
                  #cluster: cluster-conf --> NetworkAttachmentDefinition object name in Multus
                # Provide internet protocol version. IPv6, IPv4 or empty string are valid options. Empty string would mean IPv4
                #ipFamily: "IPv6"
                # Ceph daemons to listen on both IPv4 and Ipv6 networks
                #dualStack: false
              # enable the crash collector for ceph daemon crash collection
              crashCollector:
                disable: false
                # Uncomment daysToRetain to prune ceph crash entries older than the
                # specified number of days.
                #daysToRetain: 30
              # enable log collector, daemons will log on files and rotate
              # logCollector:
              #   enabled: true
              #   periodicity: 24h # SUFFIX may be 'h' for hours or 'd' for days.
              # automate [data cleanup process](https://github.com/rook/rook/blob/master/Documentation/ceph-teardown.md#delete-the-data-on-hosts) in cluster destruction.
              cleanupPolicy:
                # Since cluster cleanup is destructive to data, confirmation is required.
                # To destroy all Rook data on hosts during uninstall, confirmation must be set to "yes-really-destroy-data".
                # This value should only be set when the cluster is about to be deleted. After the confirmation is set,
                # Rook will immediately stop configuring the cluster and only wait for the delete command.
                # If the empty string is set, Rook will not destroy any data on hosts during uninstall.
                confirmation: ""
                # sanitizeDisks represents settings for sanitizing OSD disks on cluster deletion
                sanitizeDisks:
                  # method indicates if the entire disk should be sanitized or simply ceph's metadata
                  # in both case, re-install is possible
                  # possible choices are 'complete' or 'quick' (default)
                  method: quick
                  # dataSource indicate where to get random bytes from to write on the disk
                  # possible choices are 'zero' (default) or 'random'
                  # using random sources will consume entropy from the system and will take much more time then the zero source
                  dataSource: zero
                  # iteration overwrite N times instead of the default (1)
                  # takes an integer value
                  iteration: 1
                # allowUninstallWithVolumes defines how the uninstall should be performed
                # If set to true, cephCluster deletion does not wait for the PVs to be deleted.
                allowUninstallWithVolumes: false
              # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
              # The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=storage-node' and
              # tolerate taints with a key of 'storage-node'.
            #  placement:
            #    all:
            #      nodeAffinity:
            #        requiredDuringSchedulingIgnoredDuringExecution:
            #          nodeSelectorTerms:
            #          - matchExpressions:
            #            - key: role
            #              operator: In
            #              values:
            #              - storage-node
            #      podAffinity:
            #      podAntiAffinity:
            #      topologySpreadConstraints:
            #      tolerations:
            #      - key: storage-node
            #        operator: Exists
            # The above placement information can also be specified for mon, osd, and mgr components
            #    mon:
            # Monitor deployments may contain an anti-affinity rule for avoiding monitor
            # collocation on the same node. This is a required rule when host network is used
            # or when AllowMultiplePerNode is false. Otherwise this anti-affinity rule is a
            # preferred rule with weight: 50.
            #    osd:
            #    mgr:
            #    cleanup:
              annotations:
            #    all:
            #    mon:
            #    osd:
            #    cleanup:
            #    prepareosd:
            # If no mgr annotations are set, prometheus scrape annotations will be set by default.
            #    mgr:
              labels:
            #    all:
            #    mon:
            #    osd:
            #    cleanup:
            #    mgr:
            #    prepareosd:
            # monitoring is a list of key-value pairs. It is injected into all the monitoring resources created by operator.
            # These labels can be passed as LabelSelector to Prometheus
            #    monitoring:
            #    crashcollector:
              resources:
            # The requests and limits set here, allow the mgr pod to use half of one CPU core and 1 gigabyte of memory
            #    mgr:
            #      limits:
            #        cpu: "500m"
            #        memory: "1024Mi"
            #      requests:
            #        cpu: "500m"
            #        memory: "1024Mi"
            # The above example requests/limits can also be added to the other components
            #    mon:
            #    osd:
            # For OSD it also is a possible to specify requests/limits based on device class
            #    osd-hdd:
            #    osd-ssd:
            #    osd-nvme:
            #    prepareosd:
            #    mgr-sidecar:
            #    crashcollector:
            #    logcollector:
            #    cleanup:
              # The option to automatically remove OSDs that are out and are safe to destroy.
              removeOSDsIfOutAndSafeToRemove: true
            #  priorityClassNames:
            #    all: rook-ceph-default-priority-class
            #    mon: rook-ceph-mon-priority-class
            #    osd: rook-ceph-osd-priority-class
            #    mgr: rook-ceph-mgr-priority-class
              storage: # cluster level storage configuration and selection
                useAllNodes: true
                useAllDevices: true
                #deviceFilter:
                config:
                  # crushRoot: "custom-root" # specify a non-default root label for the CRUSH map
                  # metadataDevice: "md0" # specify a non-rotational storage so ceph-volume will use it as block db device of bluestore.
                  # databaseSizeMB: "1024" # uncomment if the disks are smaller than 100 GB
                  # journalSizeMB: "1024"  # uncomment if the disks are 20 GB or smaller
                  # osdsPerDevice: "1" # this value can be overridden at the node or device level
                  # encryptedDevice: "true" # the default value for this option is "false"
            # Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named
            # nodes below will be used as storage resources.  Each node's 'name' field should match their 'kubernetes.io/hostname' label.
                # nodes:
                #   - name: "172.17.4.201"
                #     devices: # specific devices to use for storage can be specified for each node
                #       - name: "sdb"
                #       - name: "nvme01" # multiple osds can be created on high performance devices
                #         config:
                #           osdsPerDevice: "5"
                #       - name: "/dev/disk/by-id/ata-ST4000DM004-XXXX" # devices can be specified using full udev paths
                #     config: # configuration can be specified at the node level which overrides the cluster level config
                #   - name: "172.17.4.301"
                #     deviceFilter: "^sd."
                # when onlyApplyOSDPlacement is false, will merge both placement.All() and placement.osd
                onlyApplyOSDPlacement: false
              # The section for configuring management of daemon disruptions during upgrade or fencing.
              disruptionManagement:
                # If true, the operator will create and manage PodDisruptionBudgets for OSD, Mon, RGW, and MDS daemons. OSD PDBs are managed dynamically
                # via the strategy outlined in the [design](https://github.com/rook/rook/blob/master/design/ceph/ceph-managed-disruptionbudgets.md). The operator will
                # block eviction of OSDs by default and unblock them safely when drains are detected.
                managePodBudgets: true
                # A duration in minutes that determines how long an entire failureDomain like `region/zone/host` will be held in `noout` (in addition to the
                # default DOWN/OUT interval) when it is draining. This is only relevant when  `managePodBudgets` is `true`. The default value is `30` minutes.
                osdMaintenanceTimeout: 30
                # A duration in minutes that the operator will wait for the placement groups to become healthy (active+clean) after a drain was completed and OSDs came back up.
                # Operator will continue with the next drain if the timeout exceeds. It only works if `managePodBudgets` is `true`.
                # No values or 0 means that the operator will wait until the placement groups are healthy before unblocking the next drain.
                pgHealthCheckTimeout: 0
                # If true, the operator will create and manage MachineDisruptionBudgets to ensure OSDs are only fenced when the cluster is healthy.
                # Only available on OpenShift.
                manageMachineDisruptionBudgets: false
                # Namespace in which to watch for the MachineDisruptionBudgets.
                machineDisruptionBudgetNamespace: openshift-machine-api

              # healthChecks
              # Valid values for daemons are 'mon', 'osd', 'status'
              healthCheck:
                daemonHealth:
                  mon:
                    disabled: false
                    interval: 45s
                  osd:
                    disabled: false
                    interval: 60s
                  status:
                    disabled: false
                    interval: 60s
                # Change pod liveness probe, it works for all mon,mgr,osd daemons
                livenessProbe:
                  mon:
                    disabled: false
                  mgr:
                    disabled: false
                  osd:
                    disabled: false